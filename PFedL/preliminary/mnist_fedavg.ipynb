{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, losses, optimizers, metrics\n",
    "from  tqdm import *\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "# tf.keras.backend.set_floatx('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 全局参数\n",
    "NUM_USER = 100\n",
    "np.random.seed(12)\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(filename)s[line:%(lineno)d] - %(levelname)s: %(message)s')\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可调参数\n",
    "loss_fn = losses.SparseCategoricalCrossentropy()\n",
    "metrics_list = [tf.keras.metrics.Accuracy()]\n",
    "# optimizer = optimizers.SGD(learning_rate=1e-2, )\n",
    "# optimizer = optimizers.SGD(learning_rate=1e-2, momentum=0.8)\n",
    "optimizer_class = optimizers.SGD\n",
    "E = 3\n",
    "batch_size = 20\n",
    "hidden_unit = 100\n",
    "beta = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# 加载数据\n",
    "with open('../data/mnist/train_array.pkl', 'rb') as f:\n",
    "    train_data = pickle.load(f)\n",
    "with open('../data/mnist/test_array.pkl', 'rb') as f:\n",
    "    test_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalization(x):\n",
    "    return x / np.sum(x)\n",
    "\n",
    "class MnistModel(tf.Module):\n",
    "    def __init__(self, hidden_unit):\n",
    "        \"\"\"\n",
    "\n",
    "        :param\n",
    "            hidden_unit: positive integer, dimensionality of the hidden layer.\n",
    "        \"\"\"\n",
    "        super(MnistModel, self).__init__()\n",
    "        self.hidden_unit = hidden_unit\n",
    "        self.fc_1 = layers.Dense(hidden_unit)\n",
    "        self.fc_2 = layers.Dense(10, activation='softmax')\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        x = layers.ReLU()(self.fc_1(x))\n",
    "        output = self.fc_2(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 客户端模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Client():\n",
    "    def __init__(self, id, hidden_unit, train_data={'x':[], 'y':[]}, \n",
    "                 test_data={'x':[], 'y':[]}, **kwargs):\n",
    "        self.id = id\n",
    "        self.train_data = {'x':np.array(train_data['x']), \n",
    "                           'y':np.array(train_data['y'])}\n",
    "        self.test_data = {'x':np.array(test_data['x']), \n",
    "                           'y':np.array(test_data['y'])}\n",
    "        self.train_samples = len(train_data['y'])\n",
    "        self.test_samples = len(test_data['y'])\n",
    "        self.model = MnistModel(hidden_unit)\n",
    "        self.index = np.arange(self.train_samples)\n",
    "        for key, value in kwargs.items():\n",
    "            setattr(self, key, value)\n",
    "        \n",
    "        assert hasattr(self, 'E')\n",
    "        assert hasattr(self, 'optimizer')  # TODO:这里改成opt_class\n",
    "        assert hasattr(self, 'lr')  # \n",
    "        assert hasattr(self, 'loss_fn')\n",
    "        assert hasattr(self, 'metrics')\n",
    "        self.optimizer = kwargs['optimizer'](self.lr)\n",
    "        self.init_model()\n",
    "    \n",
    "    def init_model(self):\n",
    "        init_feature = tf.cast(self.test_data[\"x\"][:5], dtype=tf.float64)\n",
    "        _ = self.model(init_feature)\n",
    "    \n",
    "    def forward(self, communication_round=None):\n",
    "        np.random.shuffle(self.index)\n",
    "        self.select_index = self.index[:self.batch_size * self.E]\n",
    "        train_set = tf.data.Dataset.from_tensor_slices((self.train_data[\"x\"][self.select_index],\n",
    "                                                        self.train_data[\"y\"][self.select_index])).batch(self.batch_size)\n",
    "        local_round = 0\n",
    "        for feature, label in train_set:\n",
    "            label = tf.cast(label, dtype=tf.int64)\n",
    "            with tf.GradientTape() as tape:\n",
    "                predict = self.model(feature)\n",
    "                loss = self.loss_fn(label, predict)\n",
    "            grads = tape.gradient(loss, self.model.trainable_variables)\n",
    "            self.optimizer.apply_gradients(zip(grads, self.model.trainable_variables))\n",
    "            local_round += 1\n",
    "        return (self.model.trainable_variables)\n",
    "    \n",
    "    def update_variables(self, new_variables):\n",
    "        \"\"\"\n",
    "        :param\n",
    "            new_variables: tuple, each element is ndarray\n",
    "        :result: None, only copy server model\n",
    "        \"\"\"\n",
    "        assert len(self.model.trainable_variables) == len(new_variables)\n",
    "        for i in range(len(new_variables)):\n",
    "            self.model.trainable_variables[i].assign(new_variables[i])\n",
    "\n",
    "    \n",
    "    def test(self):\n",
    "        test_set = tf.data.Dataset.from_tensor_slices((self.test_data[\"x\"], self.test_data[\"y\"])).batch(self.test_samples)\n",
    "        train_set = tf.data.Dataset.from_tensor_slices((self.train_data[\"x\"], self.train_data[\"y\"])).batch(self.train_samples)\n",
    "        for feature, label in test_set:\n",
    "            label = tf.cast(label, dtype=tf.int64)\n",
    "            output = self.model(feature)\n",
    "            loss = self.loss_fn(label, output).numpy()\n",
    "        \n",
    "        # logit = tf.nn.softmax(output, axis=-1).numpy()\n",
    "        prediction = tf.argmax(output, axis=-1).numpy()\n",
    "\n",
    "        metric_result = []\n",
    "        for metric in self.metrics:\n",
    "            metric.reset_states()\n",
    "            _ = metric.update_state(self.test_data['y'], prediction)\n",
    "            metric_result.append(metric.result().numpy())\n",
    "        \n",
    "        for feature, label in train_set:\n",
    "            label = tf.cast(label, dtype=tf.int64)\n",
    "            output = self.model(feature)\n",
    "            train_loss = self.loss_fn(label, output).numpy()\n",
    "        # group_idx = tf.argmax(self.model.c, axis=1).numpy()[0]\n",
    "        return (loss, metric_result, train_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 服务端模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Server():\n",
    "    def __init__(self, hidden_unit, train_data, test_data, E, optimizer, loss_fn, metrics, batch_size, epoches, lr, filename='/home/dihao/code/PFedL/preliminary/Result/mnist_base_fedavg.txt'):\n",
    "        self.hidden_unit = hidden_unit\n",
    "        self.train_data = train_data\n",
    "        self.test_data = test_data\n",
    "        self.E = E\n",
    "        self.optimizer = optimizer\n",
    "        self.loss_fn = loss_fn\n",
    "        self.metrics = metrics\n",
    "        self.batch_size = batch_size\n",
    "        self.epoches = epoches\n",
    "        self.model = MnistModel(hidden_unit)\n",
    "\n",
    "        self.init_model()\n",
    "        self.lastest_model = self.get_parameters()\n",
    "        self.clients = self.setup_clients(lr)\n",
    "        self.file = filename\n",
    "\n",
    "    def init_model(self):\n",
    "        init_feature = tf.cast(self.test_data[0]['x'][:5], dtype=tf.float64)\n",
    "        _ = self.model(init_feature)\n",
    "    \n",
    "    def get_parameters(self):\n",
    "        parameter = []\n",
    "        for variable in self.model.trainable_variables:\n",
    "            parameter.append(variable.numpy())\n",
    "        return parameter\n",
    "    \n",
    "    def setup_clients(self, lr):\n",
    "        client_list =[]\n",
    "        for i in range(NUM_USER):\n",
    "            client = Client(i, self.hidden_unit, train_data=self.train_data[i], test_data=self.test_data[i],\n",
    "                                     E=self.E, optimizer=self.optimizer, loss_fn=self.loss_fn, metrics=self.metrics, \n",
    "                                     batch_size=self.batch_size, lr=lr)\n",
    "            client_list.append(client)\n",
    "            \n",
    "        return client_list\n",
    "    \n",
    "    def broadcast(self):\n",
    "        for client in self.clients:\n",
    "            client.update_variables(self.lastest_model)\n",
    "    \n",
    "    def train(self):\n",
    "        for epoch in trange(self.epoches):\n",
    "            self.broadcast()  # broadcast to all clients\n",
    "            client_solution = [client.forward(epoch) for client in self.clients]\n",
    "            self.lastest_model = self.aggragate(client_solution)\n",
    "\n",
    "            round_loss, round_metrics, train_loss = self.test()\n",
    "            with open(self.file, 'a+') as f:\n",
    "                f.write('At round {}, test loss is: {:.4f}, metrics result is {}, train loss is {}'.format(epoch, round_loss, round_metrics, train_loss))\n",
    "                f.write('\\n')\n",
    "            logging.info('At round {}, test loss is: {:.4f}, metrics result is {}, train loss is {}'.format(epoch, round_loss, round_metrics, train_loss))\n",
    "\n",
    "            \n",
    "    def aggragate(self, client_solution):\n",
    "        concate_V = [np.zeros_like(x) for x in self.lastest_model]\n",
    "        for i, solution in enumerate(client_solution):\n",
    "            client_variables = solution\n",
    "            concate_V = [concate_V[j]+client_variables[j].numpy() for j in range(len(concate_V))]\n",
    "        V = [element / NUM_USER for element in concate_V]\n",
    "        return V\n",
    "\n",
    "    def test(self):\n",
    "        loss = []\n",
    "        metrics_list = [[] * len(self.metrics)]\n",
    "        train_loss = []\n",
    "        for idx, client in enumerate(self.clients):\n",
    "            client_loss, client_metrics, client_trainloss = client.test()\n",
    "            loss.append(client_loss)\n",
    "            train_loss.append(client_trainloss)\n",
    "            for i in range(len(metrics_list)):\n",
    "                metrics_list[i].append(client_metrics[i])\n",
    "        return np.mean(loss), [np.mean(metrics_list[i]) for i in range(len(metrics_list))], np.mean(train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(12)\n",
    "tf.random.set_seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      " the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2021-02-21 00:47:56,201 - base_layer.py[line:1790] - WARNING: Layer dense_174 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer dense_176 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2021-02-21 00:47:56,244 - base_layer.py[line:1790] - WARNING: Layer dense_176 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer dense_178 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2021-02-21 00:47:56,287 - base_layer.py[line:1790] - WARNING: Layer dense_178 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer dense_180 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2021-02-21 00:47:56,342 - base_layer.py[line:1790] - WARNING: Layer dense_180 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer dense_182 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2021-02-21 00:47:56,403 - base_layer.py[line:1790] - WARNING: Layer dense_182 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer dense_184 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2021-02-21 00:47:56,464 - base_layer.py[line:1790] - WARNING: Layer dense_184 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer dense_186 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2021-02-21 00:47:56,520 - base_layer.py[line:1790] - WARNING: Layer dense_186 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer dense_188 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2021-02-21 00:47:56,563 - base_layer.py[line:1790] - WARNING: Layer dense_188 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer dense_190 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2021-02-21 00:47:56,617 - base_layer.py[line:1790] - WARNING: Layer dense_190 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer dense_192 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2021-02-21 00:47:56,678 - base_layer.py[line:1790] - WARNING: Layer dense_192 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer dense_194 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2021-02-21 00:47:56,720 - base_layer.py[line:1790] - WARNING: Layer dense_194 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer dense_196 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2021-02-21 00:47:56,776 - base_layer.py[line:1790] - WARNING: Layer dense_196 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer dense_198 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2021-02-21 00:47:56,839 - base_layer.py[line:1790] - WARNING: Layer dense_198 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer dense_200 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2021-02-21 00:47:56,898 - base_layer.py[line:1790] - WARNING: Layer dense_200 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# server = Server(hidden_unit=hidden_unit, train_data=train_data['user_data'], test_data=test_data['user_data'], E=E, optimizer=optimizer,\n",
    "# loss_fn=loss_fn, metrics=metrics_list, batch_size=64, epoches=100, lr=1e-2)\n",
    "server = Server(hidden_unit=hidden_unit, train_data=train_data['user_data'], test_data=test_data['user_data'], E=E, optimizer=optimizer_class,\n",
    "loss_fn=loss_fn, metrics=metrics_list, batch_size=batch_size, epoches=200, lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true,
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "110, test loss is: 0.6866, metrics result is [0.87937033], train loss is 0.7179213166236877\n",
      " 56%|█████▌    | 111/200 [12:41<10:15,  6.91s/it]2021-02-21 01:00:50,314 - <ipython-input-8-1cf9767f9f0e>[line:53] - INFO: At round 111, test loss is: 0.6829, metrics result is [0.8797272], train loss is 0.7135552763938904\n",
      " 56%|█████▌    | 112/200 [12:48<10:07,  6.90s/it]2021-02-21 01:00:56,953 - <ipython-input-8-1cf9767f9f0e>[line:53] - INFO: At round 112, test loss is: 0.6787, metrics result is [0.8808822], train loss is 0.7087362408638\n",
      " 56%|█████▋    | 113/200 [12:55<09:53,  6.82s/it]2021-02-21 01:01:03,969 - <ipython-input-8-1cf9767f9f0e>[line:53] - INFO: At round 113, test loss is: 0.6752, metrics result is [0.8800255], train loss is 0.7063633799552917\n",
      " 57%|█████▋    | 114/200 [13:02<09:51,  6.88s/it]2021-02-21 01:01:10,934 - <ipython-input-8-1cf9767f9f0e>[line:53] - INFO: At round 114, test loss is: 0.6723, metrics result is [0.8822718], train loss is 0.702736496925354\n",
      " 57%|█████▊    | 115/200 [13:09<09:47,  6.91s/it]2021-02-21 01:01:18,049 - <ipython-input-8-1cf9767f9f0e>[line:53] - INFO: At round 115, test loss is: 0.6686, metrics result is [0.8796795], train loss is 0.6991001963615417\n",
      " 58%|█████▊    | 116/200 [13:16<09:45,  6.97s/it]2021-02-21 01:01:24,813 - <ipython-input-8-1cf9767f9f0e>[line:53] - INFO: At round 116, test loss is: 0.6641, metrics result is [0.87976587], train loss is 0.6954087615013123\n",
      " 58%|█████▊    | 117/200 [13:22<09:33,  6.91s/it]2021-02-21 01:01:31,579 - <ipython-input-8-1cf9767f9f0e>[line:53] - INFO: At round 117, test loss is: 0.6596, metrics result is [0.8817897], train loss is 0.6911203265190125\n",
      " 59%|█████▉    | 118/200 [13:29<09:22,  6.86s/it]2021-02-21 01:01:38,906 - <ipython-input-8-1cf9767f9f0e>[line:53] - INFO: At round 118, test loss is: 0.6575, metrics result is [0.8808074], train loss is 0.6888300180435181\n",
      " 60%|█████▉    | 119/200 [13:36<09:27,  7.00s/it]2021-02-21 01:01:45,898 - <ipython-input-8-1cf9767f9f0e>[line:53] - INFO: At round 119, test loss is: 0.6538, metrics result is [0.8827022], train loss is 0.6849724054336548\n",
      " 60%|██████    | 120/200 [13:43<09:20,  7.00s/it]2021-02-21 01:01:52,901 - <ipython-input-8-1cf9767f9f0e>[line:53] - INFO: At round 120, test loss is: 0.6507, metrics result is [0.88308746], train loss is 0.6809107065200806\n",
      " 60%|██████    | 121/200 [13:50<09:13,  7.00s/it]2021-02-21 01:02:00,089 - <ipython-input-8-1cf9767f9f0e>[line:53] - INFO: At round 121, test loss is: 0.6478, metrics result is [0.8811482], train loss is 0.6788259744644165\n",
      " 61%|██████    | 122/200 [13:58<09:10,  7.06s/it]2021-02-21 01:02:06,891 - <ipython-input-8-1cf9767f9f0e>[line:53] - INFO: At round 122, test loss is: 0.6432, metrics result is [0.883658], train loss is 0.674052357673645\n",
      " 62%|██████▏   | 123/200 [14:04<08:57,  6.98s/it]2021-02-21 01:02:13,763 - <ipython-input-8-1cf9767f9f0e>[line:53] - INFO: At round 123, test loss is: 0.6416, metrics result is [0.88241583], train loss is 0.6720107793807983\n",
      " 62%|██████▏   | 124/200 [14:11<08:48,  6.95s/it]2021-02-21 01:02:20,332 - <ipython-input-8-1cf9767f9f0e>[line:53] - INFO: At round 124, test loss is: 0.6350, metrics result is [0.88444555], train loss is 0.6668581366539001\n",
      " 62%|██████▎   | 125/200 [14:18<08:32,  6.83s/it]2021-02-21 01:02:27,508 - <ipython-input-8-1cf9767f9f0e>[line:53] - INFO: At round 125, test loss is: 0.6354, metrics result is [0.8835128], train loss is 0.6666319966316223\n",
      " 63%|██████▎   | 126/200 [14:25<08:33,  6.94s/it]2021-02-21 01:02:34,324 - <ipython-input-8-1cf9767f9f0e>[line:53] - INFO: At round 126, test loss is: 0.6315, metrics result is [0.88508964], train loss is 0.6629709601402283\n",
      " 64%|██████▎   | 127/200 [14:32<08:23,  6.90s/it]2021-02-21 01:02:41,065 - <ipython-input-8-1cf9767f9f0e>[line:53] - INFO: At round 127, test loss is: 0.6285, metrics result is [0.88400954], train loss is 0.6590673923492432\n",
      " 64%|██████▍   | 128/200 [14:39<08:13,  6.85s/it]2021-02-21 01:02:48,218 - <ipython-input-8-1cf9767f9f0e>[line:53] - INFO: At round 128, test loss is: 0.6276, metrics result is [0.8835237], train loss is 0.6583744883537292\n",
      " 64%|██████▍   | 129/200 [14:46<08:12,  6.94s/it]2021-02-21 01:02:55,144 - <ipython-input-8-1cf9767f9f0e>[line:53] - INFO: At round 129, test loss is: 0.6222, metrics result is [0.8849273], train loss is 0.6537631750106812\n",
      " 65%|██████▌   | 130/200 [14:53<08:05,  6.94s/it]2021-02-21 01:03:02,094 - <ipython-input-8-1cf9767f9f0e>[line:53] - INFO: At round 130, test loss is: 0.6192, metrics result is [0.8852796], train loss is 0.6503875851631165\n",
      " 66%|██████▌   | 131/200 [15:00<07:58,  6.94s/it]2021-02-21 01:03:08,978 - <ipython-input-8-1cf9767f9f0e>[line:53] - INFO: At round 131, test loss is: 0.6176, metrics result is [0.88430613], train loss is 0.6482434868812561\n",
      " 66%|██████▌   | 132/200 [15:07<07:50,  6.92s/it]2021-02-21 01:03:16,059 - <ipython-input-8-1cf9767f9f0e>[line:53] - INFO: At round 132, test loss is: 0.6131, metrics result is [0.88506395], train loss is 0.644453763961792\n",
      " 66%|██████▋   | 133/200 [15:14<07:47,  6.97s/it]2021-02-21 01:03:22,934 - <ipython-input-8-1cf9767f9f0e>[line:53] - INFO: At round 133, test loss is: 0.6125, metrics result is [0.885029], train loss is 0.6433570384979248\n",
      " 67%|██████▋   | 134/200 [15:21<07:38,  6.94s/it]2021-02-21 01:03:29,581 - <ipython-input-8-1cf9767f9f0e>[line:53] - INFO: At round 134, test loss is: 0.6076, metrics result is [0.8873375], train loss is 0.6402220726013184\n",
      " 68%|██████▊   | 135/200 [15:27<07:25,  6.85s/it]2021-02-21 01:03:36,398 - <ipython-input-8-1cf9767f9f0e>[line:53] - INFO: At round 135, test loss is: 0.6064, metrics result is [0.8848964], train loss is 0.6370099186897278\n",
      " 68%|██████▊   | 136/200 [15:34<07:17,  6.84s/it]2021-02-21 01:03:43,257 - <ipython-input-8-1cf9767f9f0e>[line:53] - INFO: At round 136, test loss is: 0.6034, metrics result is [0.8861728], train loss is 0.6343385577201843\n",
      " 68%|██████▊   | 137/200 [15:41<07:11,  6.85s/it]2021-02-21 01:03:49,934 - <ipython-input-8-1cf9767f9f0e>[line:53] - INFO: At round 137, test loss is: 0.6007, metrics result is [0.8866367], train loss is 0.6313359141349792\n",
      " 69%|██████▉   | 138/200 [15:48<07:01,  6.80s/it]2021-02-21 01:03:57,008 - <ipython-input-8-1cf9767f9f0e>[line:53] - INFO: At round 138, test loss is: 0.5987, metrics result is [0.88524574], train loss is 0.6296561360359192\n",
      " 70%|██████▉   | 139/200 [15:55<06:59,  6.88s/it]2021-02-21 01:04:03,643 - <ipython-input-8-1cf9767f9f0e>[line:53] - INFO: At round 139, test loss is: 0.5949, metrics result is [0.88811564], train loss is 0.6263142228126526\n",
      " 70%|███████   | 140/200 [16:01<06:48,  6.81s/it]2021-02-21 01:04:10,348 - <ipython-input-8-1cf9767f9f0e>[line:53] - INFO: At round 140, test loss is: 0.5938, metrics result is [0.8855302], train loss is 0.6256688237190247\n",
      " 70%|███████   | 141/200 [16:08<06:39,  6.78s/it]2021-02-21 01:04:17,279 - <ipython-input-8-1cf9767f9f0e>[line:53] - INFO: At round 141, test loss is: 0.5913, metrics result is [0.8869391], train loss is 0.6212673187255859\n",
      " 71%|███████   | 142/200 [16:15<06:35,  6.82s/it]2021-02-21 01:04:24,246 - <ipython-input-8-1cf9767f9f0e>[line:53] - INFO: At round 142, test loss is: 0.5894, metrics result is [0.8863005], train loss is 0.6204988956451416\n",
      " 72%|███████▏  | 143/200 [16:22<06:31,  6.87s/it]2021-02-21 01:04:31,292 - <ipython-input-8-1cf9767f9f0e>[line:53] - INFO: At round 143, test loss is: 0.5870, metrics result is [0.8875914], train loss is 0.6181830763816833\n",
      " 72%|███████▏  | 144/200 [16:29<06:27,  6.92s/it]2021-02-21 01:04:37,719 - <ipython-input-8-1cf9767f9f0e>[line:53] - INFO: At round 144, test loss is: 0.5836, metrics result is [0.8868338], train loss is 0.6142479181289673\n",
      " 72%|███████▎  | 145/200 [16:35<06:12,  6.77s/it]2021-02-21 01:04:44,487 - <ipython-input-8-1cf9767f9f0e>[line:53] - INFO: At round 145, test loss is: 0.5818, metrics result is [0.8871508], train loss is 0.6135675311088562\n",
      " 73%|███████▎  | 146/200 [16:42<06:05,  6.77s/it]2021-02-21 01:04:51,371 - <ipython-input-8-1cf9767f9f0e>[line:53] - INFO: At round 146, test loss is: 0.5780, metrics result is [0.8886555], train loss is 0.6091106534004211\n",
      " 74%|███████▎  | 147/200 [16:49<06:00,  6.80s/it]2021-02-21 01:04:58,625 - <ipython-input-8-1cf9767f9f0e>[line:53] - INFO: At round 147, test loss is: 0.5757, metrics result is [0.8887637], train loss is 0.6066765785217285\n",
      " 74%|███████▍  | 148/200 [16:56<06:00,  6.94s/it]2021-02-21 01:05:05,744 - <ipython-input-8-1cf9767f9f0e>[line:53] - INFO: At round 148, test loss is: 0.5757, metrics result is [0.88949037], train loss is 0.6070680022239685\n",
      " 74%|███████▍  | 149/200 [17:03<05:56,  6.99s/it]2021-02-21 01:05:12,346 - <ipython-input-8-1cf9767f9f0e>[line:53] - INFO: At round 149, test loss is: 0.5722, metrics result is [0.8880565], train loss is 0.6033123731613159\n",
      " 75%|███████▌  | 150/200 [17:10<05:43,  6.88s/it]2021-02-21 01:05:19,417 - <ipython-input-8-1cf9767f9f0e>[line:53] - INFO: At round 150, test loss is: 0.5707, metrics result is [0.88751334], train loss is 0.6009990572929382\n",
      " 76%|███████▌  | 151/200 [17:17<05:39,  6.93s/it]2021-02-21 01:05:26,331 - <ipython-input-8-1cf9767f9f0e>[line:53] - INFO: At round 151, test loss is: 0.5674, metrics result is [0.8894011], train loss is 0.5985737442970276\n",
      " 76%|███████▌  | 152/200 [17:24<05:32,  6.93s/it]2021-02-21 01:05:32,672 - <ipython-input-8-1cf9767f9f0e>[line:53] - INFO: At round 152, test loss is: 0.5680, metrics result is [0.8872071], train loss is 0.5984150171279907\n",
      " 76%|███████▋  | 153/200 [17:30<05:17,  6.75s/it]2021-02-21 01:05:39,644 - <ipython-input-8-1cf9767f9f0e>[line:53] - INFO: At round 153, test loss is: 0.5646, metrics result is [0.89000463], train loss is 0.5955344438552856\n",
      " 77%|███████▋  | 154/200 [17:37<05:13,  6.82s/it]2021-02-21 01:05:46,511 - <ipython-input-8-1cf9767f9f0e>[line:53] - INFO: At round 154, test loss is: 0.5624, metrics result is [0.89039576], train loss is 0.593550443649292\n",
      " 78%|███████▊  | 155/200 [17:44<05:07,  6.83s/it]2021-02-21 01:05:53,525 - <ipython-input-8-1cf9767f9f0e>[line:53] - INFO: At round 155, test loss is: 0.5601, metrics result is [0.88917184], train loss is 0.5912152528762817\n",
      " 78%|███████▊  | 156/200 [17:51<05:03,  6.89s/it]2021-02-21 01:05:59,769 - <ipython-input-8-1cf9767f9f0e>[line:53] - INFO: At round 156, test loss is: 0.5573, metrics result is [0.8894571], train loss is 0.5876748561859131\n",
      " 78%|███████▊  | 157/200 [17:57<04:47,  6.69s/it]2021-02-21 01:06:06,999 - <ipython-input-8-1cf9767f9f0e>[line:53] - INFO: At round 157, test loss is: 0.5549, metrics result is [0.8911787], train loss is 0.5856583118438721\n",
      " 79%|███████▉  | 158/200 [18:05<04:47,  6.86s/it]2021-02-21 01:06:13,479 - <ipython-input-8-1cf9767f9f0e>[line:53] - INFO: At round 158, test loss is: 0.5527, metrics result is [0.8901784], train loss is 0.5831902027130127\n",
      " 80%|███████▉  | 159/200 [18:11<04:36,  6.74s/it]2021-02-21 01:06:19,978 - <ipython-input-8-1cf9767f9f0e>[line:53] - INFO: At round 159, test loss is: 0.5526, metrics result is [0.8906928], train loss is 0.5831160545349121\n",
      " 80%|████████  | 160/200 [18:18<04:26,  6.67s/it]2021-02-21 01:06:26,833 - <ipython-input-8-1cf9767f9f0e>[line:53] - INFO: At round 160, test loss is: 0.5496, metrics result is [0.8905056], train loss is 0.5800265669822693\n",
      " 80%|████████  | 161/200 [18:24<04:22,  6.72s/it]2021-02-21 01:06:33,436 - <ipython-input-8-1cf9767f9f0e>[line:53] - INFO: At round 161, test loss is: 0.5471, metrics result is [0.89224946], train loss is 0.5778758525848389\n",
      " 81%|████████  | 162/200 [18:31<04:14,  6.69s/it]2021-02-21 01:06:40,404 - <ipython-input-8-1cf9767f9f0e>[line:53] - INFO: At round 162, test loss is: 0.5448, metrics result is [0.89157873], train loss is 0.5767570734024048\n",
      " 82%|████████▏ | 163/200 [18:38<04:10,  6.77s/it]2021-02-21 01:06:47,634 - <ipython-input-8-1cf9767f9f0e>[line:53] - INFO: At round 163, test loss is: 0.5425, metrics result is [0.8929248], train loss is 0.5731630325317383\n",
      " 82%|████████▏ | 164/200 [18:45<04:08,  6.91s/it]2021-02-21 01:06:54,427 - <ipython-input-8-1cf9767f9f0e>[line:53] - INFO: At round 164, test loss is: 0.5408, metrics result is [0.8934413], train loss is 0.5729040503501892\n",
      " 82%|████████▎ | 165/200 [18:52<04:00,  6.87s/it]2021-02-21 01:07:01,043 - <ipython-input-8-1cf9767f9f0e>[line:53] - INFO: At round 165, test loss is: 0.5399, metrics result is [0.89172095], train loss is 0.5710341930389404\n",
      " 83%|████████▎ | 166/200 [18:59<03:51,  6.80s/it]2021-02-21 01:07:08,537 - <ipython-input-8-1cf9767f9f0e>[line:53] - INFO: At round 166, test loss is: 0.5369, metrics result is [0.89223117], train loss is 0.567798912525177\n",
      " 84%|████████▎ | 167/200 [19:06<03:51,  7.01s/it]2021-02-21 01:07:15,975 - <ipython-input-8-1cf9767f9f0e>[line:53] - INFO: At round 167, test loss is: 0.5360, metrics result is [0.8919877], train loss is 0.5664750933647156\n",
      " 84%|████████▍ | 168/200 [19:14<03:48,  7.14s/it]2021-02-21 01:07:23,203 - <ipython-input-8-1cf9767f9f0e>[line:53] - INFO: At round 168, test loss is: 0.5363, metrics result is [0.8908048], train loss is 0.5676073431968689\n",
      " 84%|████████▍ | 169/200 [19:21<03:42,  7.16s/it]2021-02-21 01:07:30,474 - <ipython-input-8-1cf9767f9f0e>[line:53] - INFO: At round 169, test loss is: 0.5331, metrics result is [0.8914077], train loss is 0.5638867616653442\n",
      " 85%|████████▌ | 170/200 [19:28<03:35,  7.20s/it]2021-02-21 01:07:37,206 - <ipython-input-8-1cf9767f9f0e>[line:53] - INFO: At round 170, test loss is: 0.5308, metrics result is [0.8927831], train loss is 0.5620123744010925\n",
      " 86%|████████▌ | 171/200 [19:35<03:24,  7.06s/it]2021-02-21 01:07:43,692 - <ipython-input-8-1cf9767f9f0e>[line:53] - INFO: At round 171, test loss is: 0.5308, metrics result is [0.89372563], train loss is 0.5614047050476074\n",
      " 86%|████████▌ | 172/200 [19:41<03:12,  6.89s/it]2021-02-21 01:07:50,948 - <ipython-input-8-1cf9767f9f0e>[line:53] - INFO: At round 172, test loss is: 0.5264, metrics result is [0.89323103], train loss is 0.5578539371490479\n",
      " 86%|████████▋ | 173/200 [19:49<03:08,  7.00s/it]2021-02-21 01:07:57,673 - <ipython-input-8-1cf9767f9f0e>[line:53] - INFO: At round 173, test loss is: 0.5266, metrics result is [0.8940231], train loss is 0.557071328163147\n",
      " 87%|████████▋ | 174/200 [19:55<02:59,  6.92s/it]2021-02-21 01:08:04,280 - <ipython-input-8-1cf9767f9f0e>[line:53] - INFO: At round 174, test loss is: 0.5241, metrics result is [0.8932362], train loss is 0.5541991591453552\n",
      " 88%|████████▊ | 175/200 [20:02<02:50,  6.82s/it]2021-02-21 01:08:10,673 - <ipython-input-8-1cf9767f9f0e>[line:53] - INFO: At round 175, test loss is: 0.5228, metrics result is [0.8932034], train loss is 0.553615391254425\n",
      " 88%|████████▊ | 176/200 [20:08<02:40,  6.69s/it]2021-02-21 01:08:17,915 - <ipython-input-8-1cf9767f9f0e>[line:53] - INFO: At round 176, test loss is: 0.5225, metrics result is [0.8928249], train loss is 0.5524882078170776\n",
      " 88%|████████▊ | 177/200 [20:15<02:37,  6.86s/it]2021-02-21 01:08:24,675 - <ipython-input-8-1cf9767f9f0e>[line:53] - INFO: At round 177, test loss is: 0.5209, metrics result is [0.8928506], train loss is 0.5515153408050537\n",
      " 89%|████████▉ | 178/200 [20:22<02:30,  6.83s/it]2021-02-21 01:08:30,899 - <ipython-input-8-1cf9767f9f0e>[line:53] - INFO: At round 178, test loss is: 0.5200, metrics result is [0.89434075], train loss is 0.5509123206138611\n",
      " 90%|████████▉ | 179/200 [20:28<02:19,  6.65s/it]2021-02-21 01:08:37,220 - <ipython-input-8-1cf9767f9f0e>[line:53] - INFO: At round 179, test loss is: 0.5169, metrics result is [0.8931331], train loss is 0.5477136373519897\n",
      " 90%|█████████ | 180/200 [20:35<02:10,  6.55s/it]2021-02-21 01:08:43,231 - <ipython-input-8-1cf9767f9f0e>[line:53] - INFO: At round 180, test loss is: 0.5146, metrics result is [0.89370286], train loss is 0.5453347563743591\n",
      " 90%|█████████ | 181/200 [20:41<02:01,  6.39s/it]2021-02-21 01:08:49,962 - <ipython-input-8-1cf9767f9f0e>[line:53] - INFO: At round 181, test loss is: 0.5142, metrics result is [0.8938881], train loss is 0.5444627404212952\n",
      " 91%|█████████ | 182/200 [20:48<01:56,  6.49s/it]2021-02-21 01:08:55,949 - <ipython-input-8-1cf9767f9f0e>[line:53] - INFO: At round 182, test loss is: 0.5141, metrics result is [0.8944355], train loss is 0.54392009973526\n",
      " 92%|█████████▏| 183/200 [20:54<01:47,  6.34s/it]2021-02-21 01:09:02,686 - <ipython-input-8-1cf9767f9f0e>[line:53] - INFO: At round 183, test loss is: 0.5112, metrics result is [0.8940163], train loss is 0.5413370728492737\n",
      " 92%|█████████▏| 184/200 [21:00<01:43,  6.46s/it]2021-02-21 01:09:09,615 - <ipython-input-8-1cf9767f9f0e>[line:53] - INFO: At round 184, test loss is: 0.5086, metrics result is [0.8943182], train loss is 0.5391625165939331\n",
      " 92%|█████████▎| 185/200 [21:07<01:38,  6.60s/it]2021-02-21 01:09:15,969 - <ipython-input-8-1cf9767f9f0e>[line:53] - INFO: At round 185, test loss is: 0.5086, metrics result is [0.8941111], train loss is 0.5391817092895508\n",
      " 93%|█████████▎| 186/200 [21:14<01:31,  6.53s/it]2021-02-21 01:09:22,730 - <ipython-input-8-1cf9767f9f0e>[line:53] - INFO: At round 186, test loss is: 0.5061, metrics result is [0.8948214], train loss is 0.5364515781402588\n",
      " 94%|█████████▎| 187/200 [21:20<01:25,  6.60s/it]2021-02-21 01:09:29,830 - <ipython-input-8-1cf9767f9f0e>[line:53] - INFO: At round 187, test loss is: 0.5052, metrics result is [0.89619833], train loss is 0.5357745289802551\n",
      " 94%|█████████▍| 188/200 [21:27<01:20,  6.75s/it]2021-02-21 01:09:36,783 - <ipython-input-8-1cf9767f9f0e>[line:53] - INFO: At round 188, test loss is: 0.5054, metrics result is [0.894129], train loss is 0.535632848739624\n",
      " 94%|█████████▍| 189/200 [21:34<01:14,  6.81s/it]2021-02-21 01:09:43,743 - <ipython-input-8-1cf9767f9f0e>[line:53] - INFO: At round 189, test loss is: 0.5029, metrics result is [0.89482886], train loss is 0.533311128616333\n",
      " 95%|█████████▌| 190/200 [21:41<01:08,  6.85s/it]2021-02-21 01:09:50,654 - <ipython-input-8-1cf9767f9f0e>[line:53] - INFO: At round 190, test loss is: 0.5000, metrics result is [0.89590514], train loss is 0.5297814011573792\n",
      " 96%|█████████▌| 191/200 [21:48<01:01,  6.87s/it]2021-02-21 01:09:57,696 - <ipython-input-8-1cf9767f9f0e>[line:53] - INFO: At round 191, test loss is: 0.4988, metrics result is [0.8961662], train loss is 0.5292763710021973\n",
      " 96%|█████████▌| 192/200 [21:55<00:55,  6.92s/it]2021-02-21 01:10:05,135 - <ipython-input-8-1cf9767f9f0e>[line:53] - INFO: At round 192, test loss is: 0.4983, metrics result is [0.89571464], train loss is 0.5289421081542969\n",
      " 96%|█████████▋| 193/200 [22:03<00:49,  7.08s/it]2021-02-21 01:10:11,958 - <ipython-input-8-1cf9767f9f0e>[line:53] - INFO: At round 193, test loss is: 0.4973, metrics result is [0.89664954], train loss is 0.528225839138031\n",
      " 97%|█████████▋| 194/200 [22:10<00:42,  7.00s/it]2021-02-21 01:10:18,982 - <ipython-input-8-1cf9767f9f0e>[line:53] - INFO: At round 194, test loss is: 0.4969, metrics result is [0.8957666], train loss is 0.5273861885070801\n",
      " 98%|█████████▊| 195/200 [22:17<00:35,  7.01s/it]2021-02-21 01:10:25,653 - <ipython-input-8-1cf9767f9f0e>[line:53] - INFO: At round 195, test loss is: 0.4938, metrics result is [0.8965271], train loss is 0.5243484973907471\n",
      " 98%|█████████▊| 196/200 [22:23<00:27,  6.91s/it]2021-02-21 01:10:32,141 - <ipython-input-8-1cf9767f9f0e>[line:53] - INFO: At round 196, test loss is: 0.4924, metrics result is [0.89717424], train loss is 0.52309650182724\n",
      " 98%|█████████▊| 197/200 [22:30<00:20,  6.78s/it]2021-02-21 01:10:39,808 - <ipython-input-8-1cf9767f9f0e>[line:53] - INFO: At round 197, test loss is: 0.4911, metrics result is [0.89706284], train loss is 0.5213544964790344\n",
      " 99%|█████████▉| 198/200 [22:37<00:14,  7.05s/it]2021-02-21 01:10:46,529 - <ipython-input-8-1cf9767f9f0e>[line:53] - INFO: At round 198, test loss is: 0.4904, metrics result is [0.89565516], train loss is 0.5201715230941772\n",
      "100%|█████████▉| 199/200 [22:44<00:06,  6.95s/it]2021-02-21 01:10:52,928 - <ipython-input-8-1cf9767f9f0e>[line:53] - INFO: At round 199, test loss is: 0.4904, metrics result is [0.8968155], train loss is 0.5209303498268127\n",
      "100%|██████████| 200/200 [22:50<00:00,  6.85s/it]\n"
     ]
    }
   ],
   "source": [
    "server.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}