#!/bin/bash  
cd ../../

python -u train_pfl.py --dataset stackoverflow --train_batch_size 64 --eval_batch_size 1024  --num_communication_rounds 500 --num_clients_per_round 50 --num_local_epochs 1 --clip_grad_norm  --log_test_every_n_rounds 50 --max_num_clients_for_logging 1000  --arch_size mini --server_optimizer adam --server_lr 5e-5 --client_lr 0.1 --client_scheduler const --client_optimizer sgd --global_scheduler linear --global_warmup_fraction 0.1 --pretrained_model_path ./checkpoint/pfl/pretrain_model/stackoverflow_pretrain_1000/checkpoint.pt --seed 1 --pfl_algo fedalt --personalize_on_client tr_layer --layers_to_finetune 3 --logfilename ./checkpoint/pfl/train_output/stackoverflow_pfl --savedir ./checkpoint/pfl/train_model/stackoverflow_pfl 

python -u train_pfl.py --dataset stackoverflow --train_batch_size 64 --eval_batch_size 1024  --num_communication_rounds 500 --num_clients_per_round 50 --num_local_epochs 1 --clip_grad_norm  --log_test_every_n_rounds 50 --max_num_clients_for_logging 1000 --arch_size mini --server_optimizer adam --server_lr 5e-5 --client_lr 0.1 --client_scheduler const --client_optimizer sgd --global_scheduler linear --global_warmup_fraction 0.1 --pretrained_model_path ./checkpoint/pfl/pretrain_model/stackoverflow_pretrain_1000/checkpoint.pt --seed 1 --pfl_algo pfedme --pfedme_l2_reg_coef 0.001 --logfilename ./checkpoint/pfl/train_output/stackoverflow_pfedme --savedir ./checkpoint/pfl/train_model/stackoverflow_pfedme
