{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from IPython.display import clear_output\n",
    "import os\n",
    "from copy import deepcopy\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from helper import ExperimentLogger, display_train_stats\n",
    "from fl_devices import Server, Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seed\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define subdataset\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.transforms import ToTensor, Normalize, Compose\n",
    "from PIL import Image\n",
    "\n",
    "class SubCIFAR10(Dataset):\n",
    "    \"\"\"\n",
    "    Constructs a subset of CIFAR10 dataset from a pickle file;\n",
    "    expects pickle file to store list of indices\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    indices: iterable of integers\n",
    "    transform\n",
    "    data\n",
    "    targets\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    __init__\n",
    "    __len__\n",
    "    __getitem__\n",
    "    \"\"\"\n",
    "    def __init__(self, indices, cifar10_data=None, cifar10_targets=None, transform=None):\n",
    "        \"\"\"\n",
    "        :param path: path to .pkl file; expected to store list of indices\n",
    "        :param cifar10_data: Cifar-10 dataset inputs stored as torch.tensor\n",
    "        :param cifar10_targets: Cifar-10 dataset labels stored as torch.tensor\n",
    "        :param transform:\n",
    "        \"\"\"\n",
    "        # with open(path, \"rb\") as f:\n",
    "        self.indices = indices\n",
    "\n",
    "        if transform is None:\n",
    "            self.transform = \\\n",
    "                Compose([\n",
    "                    ToTensor(),\n",
    "                    Normalize(\n",
    "                        (0.4914, 0.4822, 0.4465),\n",
    "                        (0.2023, 0.1994, 0.2010)\n",
    "                    )\n",
    "                ])\n",
    "\n",
    "\n",
    "        self.data, self.targets = cifar10_data, cifar10_targets\n",
    "\n",
    "        self.data = self.data[self.indices]\n",
    "        self.targets = self.targets[self.indices]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.size(0)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img, target = self.data[index], self.targets[index]\n",
    "\n",
    "        img = Image.fromarray(img.numpy())\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        target = target\n",
    "\n",
    "        return img, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download and concate dataset\n",
    "from torchvision.datasets import CIFAR10\n",
    "train_dataset = CIFAR10('raw_data', download=True, train=True)\n",
    "test_dataset = CIFAR10('raw_data', download=True, train=False)\n",
    "\n",
    "data =  torch.cat([\n",
    "    torch.tensor(train_dataset.data),\n",
    "    torch.tensor(test_dataset.data)\n",
    "])\n",
    "\n",
    "targets = torch.cat([\n",
    "    torch.tensor(train_dataset.targets),\n",
    "    torch.tensor(test_dataset.targets)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build client datasets\n",
    "N_CLIENTS = 80\n",
    "import pickle\n",
    "\n",
    "\n",
    "client_train_dataset = []\n",
    "test_indices = []\n",
    "base_path = '../data/cifar10/all_data/train'\n",
    "for task_id, task_dir in enumerate(os.listdir(base_path)):\n",
    "    data_path = os.path.join(base_path, task_dir, 'train.pkl')\n",
    "    with open(data_path, 'rb') as f:\n",
    "        indices = pickle.load(f)\n",
    "    client_dataset = SubCIFAR10(indices, data, targets)\n",
    "    client_train_dataset.append(client_dataset)\n",
    "\n",
    "    data_path = os.path.join(base_path, task_dir, 'test.pkl')\n",
    "    with open(data_path, 'rb') as f:\n",
    "        indices = pickle.load(f)\n",
    "    test_indices.extend(indices)\n",
    "test_dataset = SubCIFAR10(test_indices, data, targets)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torchvision.models as tvmodels\n",
    "\n",
    "class Cifar10_Net(nn.Module):\n",
    "    def __init__(self, num_classes) -> None:\n",
    "        super(Cifar10_Net, self).__init__()\n",
    "        self.model = tvmodels.mobilenet_v2(pretrained=True)\n",
    "        self.model.classifier[1] = nn.Linear(self.model.classifier[1].in_features, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FL process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clients = [Client(Cifar10_Net, lambda x : torch.optim.SGD(x, lr=0.01, momentum=0.9, weight_decay=5e-4), dat, idnum=i) \n",
    "           for i, dat in enumerate(client_train_dataset)]\n",
    "server = Server(Cifar10_Net, test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "COMMUNICATION_ROUNDS = 200\n",
    "EPS_1 = 0.15\n",
    "EPS_2 = 7 # most case\n",
    "    \n",
    "    \n",
    "cfl_stats = ExperimentLogger()\n",
    "    \n",
    "cluster_indices = [np.arange(len(clients)).astype(\"int\")]\n",
    "client_clusters = [[clients[i] for i in idcs] for idcs in cluster_indices]\n",
    "\n",
    "\n",
    "for c_round in range(1, COMMUNICATION_ROUNDS+1):\n",
    "\n",
    "    if c_round == 1:\n",
    "        for client in clients:\n",
    "            client.synchronize_with_server(server)\n",
    "            \n",
    "    participating_clients = server.select_clients(clients, frac=1.0)\n",
    "\n",
    "    for client in participating_clients:\n",
    "        train_stats = client.compute_weight_update(epochs=1)\n",
    "        client.reset()\n",
    "\n",
    "    similarities = server.compute_pairwise_similarities(clients)\n",
    "\n",
    "    cluster_indices_new = []\n",
    "    for idc in cluster_indices:\n",
    "        max_norm = server.compute_max_update_norm([clients[i] for i in idc])\n",
    "        mean_norm = server.compute_mean_update_norm([clients[i] for i in idc])\n",
    "             \n",
    "        if mean_norm<EPS_1 and max_norm>EPS_2 and len(idc)>2 and c_round>20:\n",
    "            \n",
    "            server.cache_model(idc, clients[idc[0]].W, acc_clients)\n",
    "            \n",
    "            c1, c2 = server.cluster_clients(similarities[idc][:,idc]) \n",
    "            cluster_indices_new += [c1, c2]\n",
    "             \n",
    "            cfl_stats.log({\"split\" : c_round})\n",
    "\n",
    "        else:\n",
    "            cluster_indices_new += [idc]\n",
    "        \n",
    "        \n",
    "    cluster_indices = cluster_indices_new\n",
    "    client_clusters = [[clients[i] for i in idcs] for idcs in cluster_indices]\n",
    "\n",
    "    server.aggregate_clusterwise(client_clusters)\n",
    "\n",
    "    acc_clients = [client.evaluate() for client in clients]\n",
    "    \n",
    "    cfl_stats.log({\"acc_clients\" : acc_clients, \"mean_norm\" : mean_norm, \"max_norm\" : max_norm,\n",
    "                  \"rounds\" : c_round, \"clusters\" : cluster_indices})\n",
    "    \n",
    "    \n",
    "    display_train_stats(cfl_stats, EPS_1, EPS_2, COMMUNICATION_ROUNDS)\n",
    "\n",
    "    \n",
    "for idc in cluster_indices:    \n",
    "    server.cache_model(idc, clients[idc[0]].W, acc_clients)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# save result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "path = \"../plot/cfl_result/cifar10\"\n",
    "os.makedirs(path, exist_ok=True)\n",
    "\n",
    "with open(os.path.join(path, f\"seed_{SEED}.pkl\"), 'wb') as f:\n",
    "    pickle.dump(cfl_stats, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
